{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channel, n_classes):\n",
    "        super(U_net, self).__init__()\n",
    "        \n",
    "        #input image channels\n",
    "        self.in_channel = in_channel\n",
    "        # contractive path conv kernel size:(3x3)\n",
    "        self.c_kernel_size = 3\n",
    "        # expansive path conv kernel size:(3x3)\n",
    "        self.up_kernel_size = 2\n",
    "        \n",
    "        self.final_kernel_size = 1\n",
    "        self.max_pool_kernel_size = 2\n",
    "        self.max_pool_stride = 2\n",
    "        \n",
    "        self.deconv_stride = 2\n",
    "        \n",
    "        # activation function\n",
    "        self.c_activation_f = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # channels\n",
    "        self.n_out_channels1 = 64\n",
    "        self.n_out_channels2 = 128\n",
    "        self.n_out_channels3 = 256\n",
    "        self.n_out_channels4 = 512\n",
    "        self.n_out_channels5 = 1024\n",
    "        self.final_out_channels = n_classes\n",
    "        \n",
    "        self.n_skip_channels1 = 128\n",
    "        self.n_skip_channels2 = 256\n",
    "        self.n_skip_channels3 = 512\n",
    "        self.n_skip_channels4 = 1024\n",
    "        \n",
    "        self.c_max_pooling =  nn.MaxPool2d(self.max_pool_kernel_size, stride=self.max_pool_stride)\n",
    "        \n",
    "        ### contractive path layers ###\n",
    "        # 1th layer, size of input image : (batch, c=3, h=256, w=1600)\n",
    "        self.cont_layer1 = nn.Sequential(OrderedDict([\n",
    "           ('c_conv11', nn.Conv2d(in_channels=self.in_channel, out_channels=self.n_out_channels1, kernel_size=self.c_kernel_size)),\n",
    "           ('c_act_f11', self.c_activation_f),\n",
    "           ('c_conv12', nn.Conv2d(in_channels=self.n_out_channels1, out_channels=self.n_out_channels1, kernel_size=self.c_kernel_size)),\n",
    "           ('c_act_f12', self.c_activation_f)\n",
    "        ]))\n",
    "    \n",
    "        # 2th layer\n",
    "        self.cont_layer2 = nn.Sequential(OrderedDict([\n",
    "            ('c_conv21', nn.Conv2d(in_channels=self.n_out_channels1, out_channels=self.n_out_channels2, kernel_size=self.c_kernel_size)),\n",
    "            ('c_act_f21', self.c_activation_f),\n",
    "            ('c_conv22', nn.Conv2d(in_channels=self.n_out_channels2, out_channels=self.n_out_channels2, kernel_size=self.c_kernel_size)),\n",
    "            ('c_act_f22', self.c_activation_f)\n",
    "        ]))\n",
    "\n",
    "        # 3th layer\n",
    "        self.cont_layer3 = nn.Sequential(OrderedDict([\n",
    "            ('c_conv31', nn.Conv2d(in_channels=self.n_out_channels2, out_channels=self.n_out_channels3, kernel_size=self.c_kernel_size)),\n",
    "            ('c_act_f31', self.c_activation_f),\n",
    "            ('c_conv32', nn.Conv2d(in_channels=self.n_out_channels3, out_channels=self.n_out_channels3, kernel_size=self.c_kernel_size)),\n",
    "            ('c_act_f32', self.c_activation_f)\n",
    "        ]))\n",
    "        # 4th layer\n",
    "        self.cont_layer4 = nn.Sequential(OrderedDict([\n",
    "            ('c_conv41', nn.Conv2d(in_channels=self.n_out_channels3, out_channels=self.n_out_channels4, kernel_size=self.c_kernel_size)),\n",
    "            ('c_act_f41', self.c_activation_f),\n",
    "            ('c_conv42', nn.Conv2d(in_channels=self.n_out_channels4, out_channels=self.n_out_channels4, kernel_size=self.c_kernel_size)),\n",
    "            ('c_act_f42', self.c_activation_f)\n",
    "        ]))\n",
    "        # 5th layer\n",
    "        self.cont_layer5 = nn.Sequential(OrderedDict([\n",
    "            ('c_conv51', nn.Conv2d(in_channels=self.n_out_channels4, out_channels=self.n_out_channels5, kernel_size=self.c_kernel_size)),\n",
    "            ('c_act_f51', self.c_activation_f),\n",
    "            ('c_conv52', nn.Conv2d(in_channels=self.n_out_channels5, out_channels=self.n_out_channels5, kernel_size=self.c_kernel_size)),\n",
    "            ('c_act_f52', self.c_activation_f)\n",
    "        ]))\n",
    "        \n",
    "        ### expansive path layers ###\n",
    "        self.exp_layer5 = nn.ConvTranspose2d(in_channels=self.n_out_channels5, out_channels=self.n_out_channels4, \n",
    "                                                 kernel_size=self.up_kernel_size, stride=self.deconv_stride)\n",
    "        \n",
    "        # 4th layer\n",
    "        self.exp_layer4 = nn.Sequential(OrderedDict([\n",
    "            ('e_conv41', nn.Conv2d(in_channels=self.n_skip_channels4, out_channels=self.n_out_channels4, kernel_size=self.c_kernel_size)),\n",
    "            ('e_act_f41', self.c_activation_f),\n",
    "            ('e_conv42', nn.Conv2d(in_channels=self.n_out_channels4, out_channels=self.n_out_channels4, kernel_size=self.c_kernel_size)),\n",
    "            ('e_act_f42', self.c_activation_f),\n",
    "            ('e_up_conv4', nn.ConvTranspose2d(in_channels=self.n_out_channels4, out_channels=self.n_out_channels3, \n",
    "                                                 kernel_size=self.up_kernel_size, stride=self.deconv_stride))\n",
    "        ]))\n",
    "        \n",
    "        # 3th layer\n",
    "        self.exp_layer3 = nn.Sequential(OrderedDict([\n",
    "            ('e_conv31', nn.Conv2d(in_channels=self.n_skip_channels3, out_channels=self.n_out_channels3, kernel_size=self.c_kernel_size)),\n",
    "            ('e_act_f31', self.c_activation_f),\n",
    "            ('e_conv32', nn.Conv2d(in_channels=self.n_out_channels3, out_channels=self.n_out_channels3, kernel_size=self.c_kernel_size)),\n",
    "            ('e_act_f32', self.c_activation_f),\n",
    "            ('e_up_conv3', nn.ConvTranspose2d(in_channels=self.n_out_channels3, out_channels=self.n_out_channels2,\n",
    "                                                kernel_size=self.up_kernel_size, stride=self.deconv_stride))\n",
    "        ]))\n",
    "        \n",
    "        # 2th layer\n",
    "        self.exp_layer2 = nn.Sequential(OrderedDict([\n",
    "            ('e_conv21', nn.Conv2d(in_channels=self.n_skip_channels2, out_channels=self.n_out_channels2, kernel_size=self.c_kernel_size)),\n",
    "            ('e_act_f21', self.c_activation_f),\n",
    "            ('e_conv22', nn.Conv2d(in_channels=self.n_out_channels2, out_channels=self.n_out_channels2, kernel_size=self.c_kernel_size)),\n",
    "            ('e_act_f22', self.c_activation_f),\n",
    "            ('e_up_conv2', nn.ConvTranspose2d(in_channels=self.n_out_channels2, out_channels=self.n_out_channels1,\n",
    "                                                kernel_size=self.up_kernel_size, stride=self.deconv_stride))\n",
    "        ]))\n",
    "        \n",
    "        # 1th layer\n",
    "        self.exp_layer1 = nn.Sequential(OrderedDict([\n",
    "            ('e_conv11', nn.Conv2d(in_channels=self.n_skip_channels1, out_channels=self.n_out_channels1, kernel_size=self.c_kernel_size)),\n",
    "            ('e_act_f11', self.c_activation_f),\n",
    "            ('e_conv12', nn.Conv2d(in_channels=self.n_out_channels1, out_channels=self.n_out_channels1, kernel_size=self.c_kernel_size)),\n",
    "            ('e_act_f12', self.c_activation_f),\n",
    "            ('e_conv_f', nn.Conv2d(in_channels=self.n_out_channels1, out_channels=self.final_out_channels,\n",
    "                                   kernel_size=self.final_kernel_size))\n",
    "         ]))\n",
    "        \n",
    "        #### without drop-out implemented\n",
    "        \n",
    "        # skip operation : skip -> crop and concatenate\n",
    "        # return concat [n_batch, n_ch, x, h] [n_batch, n_ch, x, h] -> [n_batch, n_ch+n_ch, x, h]\n",
    "    def skipped_connection(self, cont_maps, exp_maps, height, width):\n",
    "        cropped_f_maps = self.crop_feature_maps(cont_maps, height, width)\n",
    "        return torch.cat((cropped_f_maps, exp_maps), 1)\n",
    "        \n",
    "    #features = [batchs, n_channels, height , width]\n",
    "    # h,w = cropí›„ image size\n",
    "    def crop_feature_maps(self, features, h, w):\n",
    "        h_old, w_old = features[0][0].size()\n",
    "        x = math.ceil((h_old - h) / 2)\n",
    "        y = math.ceil((w_old - w) / 2)\n",
    "        return features[:,:, x:(x + h), y:(y + w)]\n",
    "    \n",
    "    def contracting_path(self, x):\n",
    "        self.cont_layer1_out = self.cont_layer1(x)\n",
    "        self.cont_layer2_in = self.c_max_pooling(self.cont_layer1_out)\n",
    "        \n",
    "        self.cont_layer2_out = self.cont_layer2(self.cont_layer2_in)\n",
    "        self.cont_layer3_in = self.c_max_pooling(self.cont_layer2_out)\n",
    "        \n",
    "        self.cont_layer3_out = self.cont_layer3(self.cont_layer3_in)\n",
    "        self.cont_layer4_in = self.c_max_pooling(self.cont_layer3_out)\n",
    "        \n",
    "        self.cont_layer4_out = self.cont_layer4(self.cont_layer4_in)\n",
    "        self.cont_layer5_in = self.c_max_pooling(self.cont_layer4_out)\n",
    "        \n",
    "        self.cont_layer5_out = self.cont_layer5(self.cont_layer5_in)\n",
    "        \n",
    "        return self.cont_layer5_out\n",
    "    \n",
    "    # x = cont_layer5_out\n",
    "    def expansive_path(self, x):\n",
    "        x = self.exp_layer5(x)\n",
    "        x = self.skipped_connection(self.cont_layer4_out, x, x.size()[2], x.size()[3])\n",
    "        x = self.exp_layer4(x)\n",
    "        x = self.skipped_connection(self.cont_layer3_out, x, x.size()[2], x.size()[3])\n",
    "        x = self.exp_layer3(x)\n",
    "        x = self.skipped_connection(self.cont_layer2_out, x, x.size()[2], x.size()[3] )\n",
    "        x = self.exp_layer2(x)\n",
    "        x = self.skipped_connection(self.cont_layer1_out, x, x.size()[2], x.size()[3] )\n",
    "        x = self.exp_layer1(x)\n",
    "        return x\n",
    "   \n",
    "    # input_x has to be shape of (n_batches, n_channels, height, width) \n",
    "    def forward(self, input_x):\n",
    "        o_x = self.contracting_path(input_x)\n",
    "        o_x = self.expansive_path(o_x)\n",
    "        return o_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script model.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
